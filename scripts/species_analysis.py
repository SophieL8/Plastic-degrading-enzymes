# -*- coding: utf-8 -*-
"""Species Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v0g1MuOo3aLsgUx2RhynoQd_tcKO-Rur

# Significant species
"""

import numpy as np
import pandas as pd
from scipy.stats import pearsonr, spearmanr, t
from scipy import stats
import csv

# Helper functions
def calculate_correlation(x_values, y_values, method="pearson"):
    """Calculate Pearson or Spearman correlation coefficient."""
    if method == "pearson":
        correlation_coefficient, _ = pearsonr(x_values, y_values)
    elif method == "spearman":
        correlation_coefficient, _ = spearmanr(x_values, y_values)
    else:
        raise ValueError("Method must be either 'pearson' or 'spearman'")
    return correlation_coefficient

def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    """Calculate t-value and p-value for the correlation."""
    t_value = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    p_value = 2 * (1 - t.cdf(np.abs(t_value), degrees_of_freedom))  # Two-tailed test
    return t_value, p_value

def calculate_confidence_interval(correlation_coefficient, total_samples, confidence_level=0.95):
    """Calculate the confidence interval for the correlation coefficient."""
    # Fisher transformation
    z = 0.5 * np.log((1 + correlation_coefficient) / (1 - correlation_coefficient))
    standard_error = 1 / np.sqrt(total_samples - 3)

    # Critical value from normal distribution (1.96 for 95% CI)
    z_critical = stats.norm.ppf(1 - (1 - confidence_level) / 2)

    # Confidence interval in Fisher z-space
    z_lower = z - z_critical * standard_error
    z_upper = z + z_critical * standard_error

    # Convert back to correlation coefficient using the inverse Fisher transformation
    r_lower = (np.exp(2 * z_lower) - 1) / (np.exp(2 * z_lower) + 1)
    r_upper = (np.exp(2 * z_upper) - 1) / (np.exp(2 * z_upper) + 1)

    return r_lower, r_upper

# User-defined input files
pollution_file = "pollution_abundance.tsv"
species_file = "species_abundance.tsv"
correlation_method = "pearson"  # Change this to "spearman" for Spearman correlation

# Load pollution abundance data
pollution_df = pd.read_csv(pollution_file, sep='\t')

# Extract x_values (pollution abundance)
x_values = pollution_df["abundance"].values
total_samples = len(x_values)

# Load species abundance data, skipping first two rows
species_df = pd.read_csv(species_file, sep='\t', skiprows=2)

# Dictionary to store correlation coefficients
correlation_results = []

# Iterate over each row of the species abundance data
for _, row in species_df.iterrows():
    species_name = row.iloc[0]  # First column is species name
    y_values = row.iloc[1:].values  # species abundance values

    # Calculate correlation coefficient
    correlation_coefficient = calculate_correlation(x_values, y_values, method=correlation_method)
    correlation_results.append((species_name, correlation_coefficient))

# Convert to DataFrame and sort by correlation in descending order
df = pd.DataFrame(correlation_results, columns=["species", "Correlation"])
df = df.sort_values(by="Correlation", ascending=False)

# Calculate t-values, p-values, and confidence intervals
degrees_of_freedom = total_samples - 2
df[['t_value', 'p_value']] = df.apply(lambda row: calculate_t_value_distribution(row['Correlation'], degrees_of_freedom), axis=1, result_type='expand')
df[['conf_lower', 'conf_upper']] = df.apply(lambda row: calculate_confidence_interval(row['Correlation'], total_samples), axis=1, result_type='expand')

# Save the results to a TSV file
output_file = f"all_species_{correlation_method}_correlations.tsv"
df.to_csv(output_file, sep='\t', index=False)
print(f"{correlation_method.capitalize()} correlation results saved to {output_file}")

# Identify significant rows using Benjamini-Hochberg procedure
significant_rows = []
total_probability = 0

for _, row in df.iterrows():
    species_name = row["species"]
    correlation_coefficient = row["Correlation"]
    t_value = row["t_value"]
    p_value = row["p_value"]

    if total_probability + p_value <= 1:
        significant_rows.append((species_name, correlation_coefficient, t_value, p_value, row["conf_lower"], row["conf_upper"]))
        total_probability += p_value
    else:
        break

# Save significant results to a new TSV file including confidence intervals
sig_output_file = f"sig_{correlation_method}_species.tsv"
sig_df = pd.DataFrame(significant_rows, columns=["Species", "Correlation", "t_value", "p_value", "conf_lower", "conf_upper"])
sig_df.to_csv(sig_output_file, sep='\t', index=False)
print(f"Significant {correlation_method.capitalize()} results saved to {sig_output_file}")

# Step 1: Read the sig_pearson_species.tsv file into a dictionary with additional columns
pearson_data = {}
with open('sig_pearson_species.tsv', 'r') as pearson_file:
    reader = csv.reader(pearson_file, delimiter='\t')
    next(reader)  # Skip the header
    for row in reader:
        species = row[0]
        correlation = float(row[1])
        t_value = float(row[2])
        p_value = float(row[3])
        conf_lower = float(row[4])
        conf_upper = float(row[5])
        # Store the additional data in the dictionary
        pearson_data[species] = {
            'correlation': correlation,
            't_value': t_value,
            'p_value': p_value,
            'conf_lower': conf_lower,
            'conf_upper': conf_upper
        }

# Step 2: Read the sig_spearman_species.tsv file and find overlapping species
overlap_data = []
with open('sig_spearman_species.tsv', 'r') as spearman_file:
    reader = csv.reader(spearman_file, delimiter='\t')
    next(reader)  # Skip the header
    for row in reader:
        species = row[0]
        spearman_correlation = float(row[1])
        spearman_t_value = float(row[2])
        spearman_p_value = float(row[3])
        spearman_conf_lower = float(row[4])
        spearman_conf_upper = float(row[5])

        # Step 3: Check if the species is in both Pearson and Spearman data
        if species in pearson_data:
            # Retrieve Pearson data for the overlapping species
            pearson_info = pearson_data[species]
            pearson_correlation = pearson_info['correlation']
            pearson_t_value = pearson_info['t_value']
            pearson_p_value = pearson_info['p_value']
            pearson_conf_lower = pearson_info['conf_lower']
            pearson_conf_upper = pearson_info['conf_upper']

            # Step 4: Store the overlapping species and all correlation-related values
            overlap_data.append([
                species,
                pearson_correlation, pearson_t_value, pearson_p_value, pearson_conf_lower, pearson_conf_upper,
                spearman_correlation, spearman_t_value, spearman_p_value, spearman_conf_lower, spearman_conf_upper
            ])

# Step 5: Write the overlapping species and their correlation values to a new TSV file
with open('species_overlap.tsv', 'w', newline='') as overlap_file:
    writer = csv.writer(overlap_file, delimiter='\t')
    writer.writerow([
        'Species',
        'Pearson_Correlation', 'Pearson_t_value', 'Pearson_p_value', 'Pearson_conf_lower', 'Pearson_conf_upper',
        'Spearman_Correlation', 'Spearman_t_value', 'Spearman_p_value', 'Spearman_conf_lower', 'Spearman_conf_upper'
    ])
    for row in overlap_data:
        writer.writerow(row)

print("Overlapping species saved to 'species_overlap.tsv'")

"""#Species Analysis"""

import pandas as pd

file1_path = 'miTAG.taxonomic.profiles.release.tsv'
df1 = pd.read_csv(file1_path, delimiter='\t')

file2_path = 'mapped.txt'
df2 = pd.read_csv(file2_path, delimiter='\t', header=None, names=['ID', 'Sample', 'Value1', 'Value2'])

df_filtered = df1.iloc[:, :7]

sample_names_to_keep = df2['Sample'].unique()
columns_to_keep = [col for col in df1.columns if any(sample_name in col for sample_name in sample_names_to_keep)]
df_filtered = pd.concat([df_filtered, df1[columns_to_keep]], axis=1)

output_file_path = 'miTAG_taxonomic_filtered.tsv'
df_filtered.to_csv(output_file_path, sep='\t', index=False)

"""#### Using pearson correlation"""

import numpy as np
import scipy.stats as stats
import pandas as pd
from scipy.stats import pearsonr

# Load data
taxonomic_data = pd.read_csv('miTAG_taxonomic_filtered.tsv', delimiter='\t')
x_values = taxonomic_data.iloc[:, 7:].values

mapped_data = pd.read_csv('mapped.txt', sep='\t', header=None)

release_df = pd.read_csv("TARA243.KO.profile.release", sep='\s+', skiprows=[1])

# Calculate t-value and probability distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - stats.t.cdf(x, degrees_of_freedom)
    return x, probability

# Calculate Spearman correlation
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Degrees of freedom
degrees_of_freedom = 39

correlation_coefficients = {}

# Calculate correlation coefficients for each row
for row_index in range(len(release_df)):
    y_values = mapped_data[2].values
    x_row_values = x_values[row_index]
    correlation_coefficient = calculate_correlation(x_row_values, y_values)
    correlation_coefficients[row_index] = correlation_coefficient

# Sort the correlation coefficients in descending order
sorted_coefficients = sorted(correlation_coefficients.items(), key=lambda x: x[1], reverse=True)

# Extract taxonomic data
domain = taxonomic_data.iloc[:, 0].values
phylum = taxonomic_data.iloc[:, 1].values
specie_class = taxonomic_data.iloc[:, 2].values
order = taxonomic_data.iloc[:, 3].values
family = taxonomic_data.iloc[:, 4].values
genus = taxonomic_data.iloc[:, 5].values
specie = taxonomic_data.iloc[:, 6].values

with open("p_species_correlation.tab", "w") as file:
    file.write("Domain\tPhylum\tSpecies Class\tOrder\tFamily\tGenus\tSpecies\tCorrelation\tx\tProbability\n")

    for row_index, correlation_coefficient in sorted_coefficients:
        one = domain[row_index]
        two = phylum[row_index]
        three = specie_class[row_index]
        four = order[row_index]
        five = family[row_index]
        six = genus[row_index]
        seven = specie[row_index]

        x, probability = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        file.write(f"{one}\t{two}\t{three}\t{four}\t{five}\t{six}\t{seven}\t{correlation_coefficient}\t{x}\t{probability}\n")

# Benjamini–Hochberg method
significant_rows = []

total_probability = 0

for row_index, correlation_coefficient in sorted_coefficients:
    one = domain[row_index]
    two = phylum[row_index]
    three = specie_class[row_index]
    four = order[row_index]
    five = family[row_index]
    six = genus[row_index]
    seven = specie[row_index]
    x, probability = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

    if total_probability + probability <= 1:
        significant_rows.append((row_index, seven, correlation_coefficient, x, probability))
        total_probability += probability
    else:
        break

mapped_data = pd.read_csv('mapped.txt', sep='\t', header=None)

second_values = mapped_data[1].values
y_values = mapped_data[2].values

if significant_rows:
    significant_rows = significant_rows[:-1]

with open("p_significant_species.tab", "w") as file:
    for row_index, seven, correlation_coefficient, x, probability in significant_rows:
        one = domain[row_index]
        two = phylum[row_index]
        three = specie_class[row_index]
        four = order[row_index]
        five = family[row_index]
        six = genus[row_index]
        seven = specie[row_index]
        file.write(f"{one}\t{two}\t{three}\t{four}\t{five}\t{six}\t{seven}\t{correlation_coefficient}\t{probability}\n")
        # file.write(f"{one}\t{two}\t{three}\t{four}\t{five}\t{six}\t{seven}\t{correlation_coefficient}\t{probability}\t{x}\t{y_values}\n")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from scipy.stats import pearsonr

def plot_line_of_best_fit(x, y, color, label):
    # Fit a first-degree polynomial (line) to the data
    slope, intercept = np.polyfit(x, y, 1)

    # Plot the line of best fit
    plt.plot(x, slope * np.array(x) + intercept, color=color, linestyle='--', label=f'Fit for {label}')

colors = ['blue', 'green', 'red', 'purple', 'orange']

markers = ['o', 's', '^', 'v', 'D']

plt.figure(figsize=(10, 6))

for i in range(min(5, len(sorted_coefficients))):
    row_index, correlation_coefficient = sorted_coefficients[i]
    one = domain[row_index]
    two = phylum[row_index]
    three = specie_class[row_index]
    four = order[row_index]
    five = family[row_index]
    six = genus[row_index]
    seven = specie[row_index]

    x_row_values = x_values[row_index]
    marker = markers[i]
    color = colors[i]

    sns.scatterplot(x=x_row_values, y=y_values, label=f'Species: {seven}\nCorrelation: {correlation_coefficient}',
                    color=color, marker=marker, alpha=0.7, s=25)

    plot_line_of_best_fit(x_row_values, y_values, color, label=seven)

plt.xlabel('Species Abundance')
plt.ylabel('Pollution Degree')
plt.title('Top 5 Species Correlations')
plt.legend()
plt.show()

"""#### Using Spearman Correlation"""

import numpy as np
import scipy.stats as stats
import pandas as pd
from scipy.stats import spearmanr

# Load data
taxonomic_data = pd.read_csv('miTAG_taxonomic_filtered.tsv', delimiter='\t')
x_values = taxonomic_data.iloc[:, 7:].values

mapped_data = pd.read_csv('mapped.txt', sep='\t', header=None)

release_df = pd.read_csv("TARA243.KO.profile.release", sep='\s+', skiprows=[1])

# Calculate t-value and probability distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - stats.t.cdf(x, degrees_of_freedom)
    return x, probability

# Calculate Spearman correlation
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = spearmanr(x_values, y_values)
    return correlation_coefficient

# Degrees of freedom
degrees_of_freedom = 39

correlation_coefficients = {}

# Calculate correlation coefficients for each row
for row_index in range(len(release_df)):
    y_values = mapped_data[2].values
    x_row_values = x_values[row_index]
    correlation_coefficient = calculate_correlation(x_row_values, y_values)
    correlation_coefficients[row_index] = correlation_coefficient

# Sort the correlation coefficients in descending order
sorted_coefficients = sorted(correlation_coefficients.items(), key=lambda x: x[1], reverse=True)

# Extract taxonomic data
domain = taxonomic_data.iloc[:, 0].values
phylum = taxonomic_data.iloc[:, 1].values
specie_class = taxonomic_data.iloc[:, 2].values
order = taxonomic_data.iloc[:, 3].values
family = taxonomic_data.iloc[:, 4].values
genus = taxonomic_data.iloc[:, 5].values
specie = taxonomic_data.iloc[:, 6].values

with open("s_species_correlation.tab", "w") as file:
    file.write("Domain\tPhylum\tSpecies Class\tOrder\tFamily\tGenus\tSpecies\tCorrelation\tx\tProbability\n")

    for row_index, correlation_coefficient in sorted_coefficients:
        one = domain[row_index]
        two = phylum[row_index]
        three = specie_class[row_index]
        four = order[row_index]
        five = family[row_index]
        six = genus[row_index]
        seven = specie[row_index]

        x, probability = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        file.write(f"{one}\t{two}\t{three}\t{four}\t{five}\t{six}\t{seven}\t{correlation_coefficient}\t{x}\t{probability}\n")

# Benjamini–Hochberg method
significant_rows = []

total_probability = 0

for row_index, correlation_coefficient in sorted_coefficients:
    one = domain[row_index]
    two = phylum[row_index]
    three = specie_class[row_index]
    four = order[row_index]
    five = family[row_index]
    six = genus[row_index]
    seven = specie[row_index]
    x, probability = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

    if total_probability + probability <= 1:
        significant_rows.append((row_index, seven, correlation_coefficient, x, probability))
        total_probability += probability
    else:
        break

mapped_data = pd.read_csv('mapped.txt', sep='\t', header=None)

second_values = mapped_data[1].values
y_values = mapped_data[2].values

if significant_rows:
    significant_rows = significant_rows[:-1]

with open("s_significant_species.tab", "w") as file:
    for row_index, seven, correlation_coefficient, x, probability in significant_rows:
        one = domain[row_index]
        two = phylum[row_index]
        three = specie_class[row_index]
        four = order[row_index]
        five = family[row_index]
        six = genus[row_index]
        seven = specie[row_index]
        file.write(f"{one}\t{two}\t{three}\t{four}\t{five}\t{six}\t{seven}\t{correlation_coefficient}\t{probability}\n")
        # file.write(f"{one}\t{two}\t{three}\t{four}\t{five}\t{six}\t{seven}\t{correlation_coefficient}\t{probability}\t{x}\t{y_values}\n")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from scipy.stats import pearsonr

def plot_line_of_best_fit(x, y, color, label):
    # Fit a first-degree polynomial (line) to the data
    slope, intercept = np.polyfit(x, y, 1)

    # Plot the line of best fit
    plt.plot(x, slope * np.array(x) + intercept, color=color, linestyle='--', label=f'Fit for {label}')

colors = ['blue', 'green', 'red', 'purple', 'orange']

markers = ['o', 's', '^', 'v', 'D']

plt.figure(figsize=(10, 6))

for i in range(min(5, len(sorted_coefficients))):
    row_index, correlation_coefficient = sorted_coefficients[i]
    one = domain[row_index]
    two = phylum[row_index]
    three = specie_class[row_index]
    four = order[row_index]
    five = family[row_index]
    six = genus[row_index]
    seven = specie[row_index]

    x_row_values = x_values[row_index]
    marker = markers[i]
    color = colors[i]

    sns.scatterplot(x=x_row_values, y=y_values, label=f'Species: {seven}\nCorrelation: {correlation_coefficient}',
                    color=color, marker=marker, alpha=0.7, s=25)

    plot_line_of_best_fit(x_row_values, y_values, color, label=seven)

plt.xlabel('Species Abundance')
plt.ylabel('Pollution Degree')
plt.title('Top 5 Species Spearman Correlations')
plt.legend()
plt.show()

"""# Cross-validation (5)"""

import pandas as pd
import numpy as np
import itertools
from scipy.stats import pearsonr, t

# Function to calculate Pearson correlation coefficient
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - t.cdf(x, degrees_of_freedom)
    return x, probability

# Step 1: Load the mapped.txt file into a DataFrame
df = pd.read_csv("mapped.txt", header=None, delim_whitespace=True)

# Step 2: Randomly shuffle the samples (41 rows in total)
df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Step 3: Split the 41 rows into 5 groups of 8, 8, 8, 8, and 9
rows_per_group = [8, 8, 8, 8, 9]  # Sizes of the groups
splits = []
start_idx = 0
for count in rows_per_group:
    splits.append(df_shuffled.iloc[start_idx:start_idx + count])
    start_idx += count

# Step 4: Generate all combinations of 4 groups out of the 5 groups
group_combinations = list(itertools.combinations(range(5), 4))  # Generate combinations of 4 out of 5 groups

# Step 5: Iterate over each group combination and process
for combo_index, combo in enumerate(group_combinations):
    # Combine the selected groups
    combined_rows = pd.concat([splits[i] for i in combo], ignore_index=True)

    # Save the combined group rows to a file
    group_file_name = f"cross-combo{combo_index + 1}.tab"
    combined_rows.to_csv(group_file_name, sep='\t', index=False, header=False)

    # Step 6: Load the enzyme abundance data (release file)
    release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

    # Step 7: Read the sample names and pollution values
    sample_names = combined_rows[1].values  # Sample names from the combination file
    y_values = combined_rows[2].values  # Pollution values from the combination file

    # Step 8: Create a dictionary of enzyme data from the TARA file
    tara_dict = release_df.set_index('ko').to_dict('index')

    # Step 9: Process the combined rows for correlation calculation
    x_values_for_all_enzymes = []  # List to store x_values for all enzymes for this combination

    # Step 10: Iterate over each enzyme in the TARA file
    for enzyme, enzyme_data in tara_dict.items():
        x_values = []

        # Extract x_values corresponding to the sample names in the current group
        for sample in sample_names:
            if sample in enzyme_data:
                x_values.append(enzyme_data[sample])
            else:
                x_values.append(np.nan)  # If the sample is not found, append NaN

        # Store the x_values along with the enzyme name for future output
        x_values_for_all_enzymes.append([enzyme] + x_values)

    # Step 11: Calculate the Pearson correlation for each enzyme
    correlation_results = []
    for enzyme_data in x_values_for_all_enzymes:
        enzyme_name = enzyme_data[0]
        x_values = enzyme_data[1:]

        # Calculate the correlation between x_values (enzyme abundance) and y_values (pollution values)
        correlation_coefficient = calculate_correlation(x_values, y_values)
        correlation_results.append((enzyme_name, correlation_coefficient))

    # Step 12: Sort the correlation coefficients in descending order
    correlation_results.sort(key=lambda x: x[1], reverse=True)

    # Step 13: Save the sorted correlation results to a file
    with open(f"cross-combo{combo_index + 1}_all_correlations.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient in correlation_results:
            degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
            t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

    # Step 14: Optionally calculate and save the t-value distribution for the correlations
    degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
    significant_rows = []
    total_probability = 0

    # Check the significance of each enzyme
    for enzyme_name, correlation_coefficient in correlation_results:
        t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        # Add the current result if the total probability does not exceed 1
        if total_probability + p_value <= 1:
            significant_rows.append((enzyme_name, correlation_coefficient, t_value, p_value))
            total_probability += p_value
        else:
            break

    # Step 15: Save the significant enzymes to a file
    with open(f"cross_sig_combo{combo_index + 1}.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient, t_value, p_value in significant_rows:
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

# Finding shared enzymes between cross combos and sig_pearson_enzymes
import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Read the sig_pearson_enzymes.txt file (this is now in a specific format)
sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# List of cross_sig_combo files to process
group_files = [
    "cross_sig_combo1.tab",
    "cross_sig_combo2.tab",
    "cross_sig_combo3.tab",
    "cross_sig_combo4.tab",
    "cross_sig_combo5.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 6)]
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 6)])

# Calculate the percentage of common enzymes for each group
total_enzymes_in_sig = len(sig_enzymes)
for i in range(1, 6):
    group_column = f"Group{i} Correlation"
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()
    percentage_common = (common_enzymes_in_group / total_enzymes_in_sig) * 100 if total_enzymes_in_sig else 0
    df[f"Percentage Common in Group{i}"] = percentage_common

# Calculate the shared count (how many groups share each enzyme)
df["Shared Count"] = df[["Group1 Correlation", "Group2 Correlation", "Group3 Correlation", "Group4 Correlation", "Group5 Correlation"]].apply(lambda x: (x != "na").sum(), axis=1)

# Sort by the number of shared groups (from most to least)
df_sorted = df.sort_values(by="Shared Count", ascending=False)

# Drop the "Shared Count" column as it's just for sorting
df_sorted = df_sorted.drop(columns=["Shared Count"])

# Format the correlations to 2 decimal places for better readability
for group in ["Group1 Correlation", "Group2 Correlation", "Group3 Correlation", "Group4 Correlation", "Group5 Correlation"]:
    df_sorted[group] = df_sorted[group].apply(lambda x: f"{x:.2f}" if x != "na" else "na")

# Format the percentages to 2 decimal places for readability
for i in range(1, 6):
    df_sorted[f"Percentage Common in Group{i}"] = df_sorted[f"Percentage Common in Group{i}"].apply(lambda x: f"{x:.2f}%")

df_sorted.to_csv("cross_validation_results.tab", sep="\t", index=False)

# Calculating percentage of shared enzymes by dividing by smaller number
import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

group_files = [
    "cross_sig_combo1.tab",
    "cross_sig_combo2.tab",
    "cross_sig_combo3.tab",
    "cross_sig_combo4.tab",
    "cross_sig_combo5.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 6)]
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 6)])

# Calculate the percentage of common enzymes for each group
for i in range(1, 6):
    group_column = f"Group{i} Correlation"

    # Count the common enzymes in the current group
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()

    # Get the size of the current group and sig_enzymes
    total_enzymes_in_sig = len(sig_enzymes)
    total_enzymes_in_group = len(df)

    # Calculate the smaller number to divide by
    denominator = min(total_enzymes_in_sig, total_enzymes_in_group)

    # Calculate the percentage common
    percentage_common = (common_enzymes_in_group / denominator) * 100 if denominator else 0

    # Add the percentage common to the DataFrame
    df[f"Percentage Common in Group{i}"] = percentage_common

# Calculate the shared count (how many groups share each enzyme)
df["Shared Count"] = df[["Group1 Correlation", "Group2 Correlation", "Group3 Correlation", "Group4 Correlation", "Group5 Correlation"]].apply(lambda x: (x != "na").sum(), axis=1)

# Sort by the number of shared groups (from most to least)
df_sorted = df.sort_values(by="Shared Count", ascending=False)
df_sorted = df_sorted.drop(columns=["Shared Count"])

for group in ["Group1 Correlation", "Group2 Correlation", "Group3 Correlation", "Group4 Correlation", "Group5 Correlation"]:
    df_sorted[group] = df_sorted[group].apply(lambda x: f"{x:.2f}" if x != "na" else "na")

for i in range(1, 6):
    df_sorted[f"Percentage Common in Group{i}"] = df_sorted[f"Percentage Common in Group{i}"].apply(lambda x: f"{x:.2f}%")

df_sorted.to_csv("cross_validation_results.tab", sep="\t", index=False)

"""# Cross-validation (10)"""

import pandas as pd
import numpy as np
import itertools
from scipy.stats import pearsonr, t

# Function to calculate Pearson correlation coefficient
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - t.cdf(x, degrees_of_freedom)
    return x, probability

# Step 1: Load the mapped.txt file into a DataFrame
df = pd.read_csv("mapped.txt", header=None, delim_whitespace=True)

# Step 2: Randomly shuffle the samples (41 rows in total)
df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Step 3: Split the 41 rows into 10 groups with the specified sizes
rows_per_group = [4, 4, 4, 4, 4, 4, 4, 4, 4, 5]  # Sizes of the groups
splits = []
start_idx = 0
for count in rows_per_group:
    splits.append(df_shuffled.iloc[start_idx:start_idx + count])
    start_idx += count

# Step 4: Generate all combinations of 9 groups out of the 10 groups
group_combinations = list(itertools.combinations(range(10), 9))  # Generate combinations of 9 out of 10 groups

# Step 5: Iterate over each group combination and process
for combo_index, combo in enumerate(group_combinations):
    # Combine the selected groups
    combined_rows = pd.concat([splits[i] for i in combo], ignore_index=True)

    # Save the combined group rows to a file
    group_file_name = f"cross-combo{combo_index + 1}.tab"
    combined_rows.to_csv(group_file_name, sep='\t', index=False, header=False)

    # Step 6: Load the enzyme abundance data (release file)
    release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

    # Step 7: Read the sample names and pollution values
    sample_names = combined_rows[1].values  # Sample names from the combination file
    y_values = combined_rows[2].values  # Pollution values from the combination file

    # Step 8: Create a dictionary of enzyme data from the TARA file
    tara_dict = release_df.set_index('ko').to_dict('index')

    # Step 9: Process the combined rows for correlation calculation
    x_values_for_all_enzymes = []  # List to store x_values for all enzymes for this combination

    # Step 10: Iterate over each enzyme in the TARA file
    for enzyme, enzyme_data in tara_dict.items():
        x_values = []

        # Extract x_values corresponding to the sample names in the current group
        for sample in sample_names:
            if sample in enzyme_data:
                x_values.append(enzyme_data[sample])
            else:
                x_values.append(np.nan)  # If the sample is not found, append NaN

        # Store the x_values along with the enzyme name for future output
        x_values_for_all_enzymes.append([enzyme] + x_values)

    # Step 11: Calculate the Pearson correlation for each enzyme
    correlation_results = []
    for enzyme_data in x_values_for_all_enzymes:
        enzyme_name = enzyme_data[0]
        x_values = enzyme_data[1:]

        # Calculate the correlation between x_values (enzyme abundance) and y_values (pollution values)
        correlation_coefficient = calculate_correlation(x_values, y_values)
        correlation_results.append((enzyme_name, correlation_coefficient))

    # Step 12: Sort the correlation coefficients in descending order
    correlation_results.sort(key=lambda x: x[1], reverse=True)

    # Step 13: Save the sorted correlation results to a file
    with open(f"cross-combo{combo_index + 1}_all_correlations.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient in correlation_results:
            degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
            t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

    # Step 14: Optionally calculate and save the t-value distribution for the correlations
    degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
    significant_rows = []
    total_probability = 0

    # Check the significance of each enzyme
    for enzyme_name, correlation_coefficient in correlation_results:
        t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        # Add the current result if the total probability does not exceed 1
        if total_probability + p_value <= 1:
            significant_rows.append((enzyme_name, correlation_coefficient, t_value, p_value))
            total_probability += p_value
        else:
            break

    # Step 15: Save the significant enzymes to a file
    with open(f"cross_sig_combo{combo_index + 1}.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient, t_value, p_value in significant_rows:
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Read the sig_pearson_enzymes.txt file (this is now in a specific format)
sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# List of cross_sig_combo files to process (updated to 10 groups)
group_files = [
    "cross_sig_combo1.tab",
    "cross_sig_combo2.tab",
    "cross_sig_combo3.tab",
    "cross_sig_combo4.tab",
    "cross_sig_combo5.tab",
    "cross_sig_combo6.tab",
    "cross_sig_combo7.tab",
    "cross_sig_combo8.tab",
    "cross_sig_combo9.tab",
    "cross_sig_combo10.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 11)]  # Update to 10 groups
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 11)])  # Update to 10 groups

# Calculate the percentage of common enzymes for each group
total_enzymes_in_sig = len(sig_enzymes)
for i in range(1, 11):  # Now loop over 10 groups
    group_column = f"Group{i} Correlation"
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()
    percentage_common = (common_enzymes_in_group / total_enzymes_in_sig) * 100 if total_enzymes_in_sig else 0
    df[f"Percentage Common in Group{i}"] = percentage_common

# Calculate the shared count (how many groups share each enzyme)
df["Shared Count"] = df[["Group1 Correlation", "Group2 Correlation", "Group3 Correlation", "Group4 Correlation", "Group5 Correlation",
                         "Group6 Correlation", "Group7 Correlation", "Group8 Correlation", "Group9 Correlation", "Group10 Correlation"]].apply(
    lambda x: (x != "na").sum(), axis=1)

# Sort by the number of shared groups (from most to least)
df_sorted = df.sort_values(by="Shared Count", ascending=False)

# Drop the "Shared Count" column as it's just for sorting
df_sorted = df_sorted.drop(columns=["Shared Count"])

# Format the correlations to 2 decimal places for better readability
for group in [f"Group{i} Correlation" for i in range(1, 11)]:  # Update to 10 groups
    df_sorted[group] = df_sorted[group].apply(lambda x: f"{x:.2f}" if x != "na" else "na")

# Format the percentages to 2 decimal places for readability
for i in range(1, 11):  # Update to 10 groups
    df_sorted[f"Percentage Common in Group{i}"] = df_sorted[f"Percentage Common in Group{i}"].apply(lambda x: f"{x:.2f}%")

# Save the results to a file
df_sorted.to_csv("10cross_validation_results.tab", sep="\t", index=False)

import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# Updated list of cross_sig_combo files for 10 groups
group_files = [
    "cross_sig_combo1.tab",
    "cross_sig_combo2.tab",
    "cross_sig_combo3.tab",
    "cross_sig_combo4.tab",
    "cross_sig_combo5.tab",
    "cross_sig_combo6.tab",
    "cross_sig_combo7.tab",
    "cross_sig_combo8.tab",
    "cross_sig_combo9.tab",
    "cross_sig_combo10.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 11)]  # Update for 10 groups
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 11)])  # Updated for 10 groups

# Calculate the percentage of common enzymes for each group
percentages = {}
for i in range(1, 11):  # Now looping over 10 groups
    group_column = f"Group{i} Correlation"

    # Count the common enzymes in the current group
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()

    # Get the size of the current group and sig_enzymes
    total_enzymes_in_sig = len(sig_enzymes)
    total_enzymes_in_group = len(df)

    # Calculate the smaller number to divide by
    denominator = min(total_enzymes_in_sig, total_enzymes_in_group)

    # Calculate the percentage common
    percentage_common = (common_enzymes_in_group / denominator) * 100 if denominator else 0

    # Store the percentage in the dictionary for output
    percentages[f"Group{i}"] = percentage_common

# Convert the percentages dictionary into a DataFrame
percentages_df = pd.DataFrame(list(percentages.items()), columns=["Group", "Percentage"])

# Format the percentages to 2 decimal places
percentages_df["Percentage"] = percentages_df["Percentage"].apply(lambda x: f"{x:.5f}%")

# Save the percentages to a separate file
percentages_df.to_csv("10_cross_validation_percentages.tab", sep="\t", index=False)





import pandas as pd
import numpy as np
import itertools
from scipy.stats import pearsonr, t

# Function to calculate Pearson correlation coefficient
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - t.cdf(x, degrees_of_freedom)
    return x, probability

# Step 1: Load the mapped.txt file into a DataFrame
df = pd.read_csv("mapped.txt", header=None, delim_whitespace=True)

# Step 2: Define the specific order of rows (adjusted for 0-based indexing)
row_order = [
    38, 28, 33, 9, 0, 25, 14, 39, 23, 37, 15, 20, 11, 6, 32, 29, 21, 7, 36, 18,
    2, 10, 22, 13, 5, 40, 3, 8, 4, 24, 35, 16, 19, 27, 31, 17, 26, 34, 30, 12, 1
]

# Step 3: Reorder the rows according to the specific row order
df_ordered = df.iloc[row_order].reset_index(drop=True)

# Step 4: Split the 41 rows into 10 groups with the specified sizes
rows_per_group = [4, 4, 4, 4, 4, 4, 4, 4, 4, 5]  # Sizes of the groups
splits = []
start_idx = 0
for count in rows_per_group:
    splits.append(df_ordered.iloc[start_idx:start_idx + count])
    start_idx += count

# Step 5: Generate all combinations of 9 groups out of the 10 groups
group_combinations = list(itertools.combinations(range(10), 9))  # Generate combinations of 9 out of 10 groups

# Step 6: Iterate over each group combination and process
for combo_index, combo in enumerate(group_combinations):
    # Combine the selected groups
    combined_rows = pd.concat([splits[i] for i in combo], ignore_index=True)

    # Save the combined group rows to a file
    group_file_name = f"cross-combo{combo_index + 1}.tab"
    combined_rows.to_csv(group_file_name, sep='\t', index=False, header=False)

    # Step 7: Load the enzyme abundance data (release file)
    release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

    # Step 8: Read the sample names and pollution values
    sample_names = combined_rows[1].values  # Sample names from the combination file
    y_values = combined_rows[2].values  # Pollution values from the combination file

    # Step 9: Create a dictionary of enzyme data from the TARA file
    tara_dict = release_df.set_index('ko').to_dict('index')

    # Step 10: Process the combined rows for correlation calculation
    x_values_for_all_enzymes = []  # List to store x_values for all enzymes for this combination

    # Step 11: Iterate over each enzyme in the TARA file
    for enzyme, enzyme_data in tara_dict.items():
        x_values = []

        # Extract x_values corresponding to the sample names in the current group
        for sample in sample_names:
            if sample in enzyme_data:
                x_values.append(enzyme_data[sample])
            else:
                x_values.append(np.nan)  # If the sample is not found, append NaN

        # Store the x_values along with the enzyme name for future output
        x_values_for_all_enzymes.append([enzyme] + x_values)

    # Step 12: Calculate the Pearson correlation for each enzyme
    correlation_results = []
    for enzyme_data in x_values_for_all_enzymes:
        enzyme_name = enzyme_data[0]
        x_values = enzyme_data[1:]

        # Calculate the correlation between x_values (enzyme abundance) and y_values (pollution values)
        correlation_coefficient = calculate_correlation(x_values, y_values)
        correlation_results.append((enzyme_name, correlation_coefficient))

    # Step 13: Sort the correlation coefficients in descending order
    correlation_results.sort(key=lambda x: x[1], reverse=True)

    # Step 14: Save the sorted correlation results to a file
    with open(f"cross-combo{combo_index + 1}_all_correlations.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient in correlation_results:
            degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
            t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

    # Step 15: Optionally calculate and save the t-value distribution for the correlations
    degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
    significant_rows = []
    total_probability = 0

    # Check the significance of each enzyme
    for enzyme_name, correlation_coefficient in correlation_results:
        t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        # Add the current result if the total probability does not exceed 1
        if total_probability + p_value <= 1:
            significant_rows.append((enzyme_name, correlation_coefficient, t_value, p_value))
            total_probability += p_value
        else:
            break

    # Step 16: Save the significant enzymes to a file
    with open(f"1_10_cross_sig_combo{combo_index + 1}.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient, t_value, p_value in significant_rows:
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# Updated list of cross_sig_combo files for 10 groups
group_files = [
    "1_10_cross_sig_combo1.tab",
    "1_10_cross_sig_combo2.tab",
    "1_10_cross_sig_combo3.tab",
    "1_10_cross_sig_combo4.tab",
    "1_10_cross_sig_combo5.tab",
    "1_10_cross_sig_combo6.tab",
    "1_10_cross_sig_combo7.tab",
    "1_10_cross_sig_combo8.tab",
    "1_10_cross_sig_combo9.tab",
    "1_10_cross_sig_combo10.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 11)]  # Update for 10 groups
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 11)])  # Updated for 10 groups

# Calculate the percentage of common enzymes for each group
percentages = {}
for i in range(1, 11):  # Now looping over 10 groups
    group_column = f"Group{i} Correlation"

    # Count the common enzymes in the current group
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()

    # Get the size of the current group and sig_enzymes
    total_enzymes_in_sig = len(sig_enzymes)
    total_enzymes_in_group = len(df)

    # Calculate the smaller number to divide by
    denominator = min(total_enzymes_in_sig, total_enzymes_in_group)

    # Calculate the percentage common
    percentage_common = (common_enzymes_in_group / denominator) * 100 if denominator else 0

    # Store the percentage in the dictionary for output
    percentages[f"Group{i}"] = percentage_common

# Convert the percentages dictionary into a DataFrame
percentages_df = pd.DataFrame(list(percentages.items()), columns=["Group", "Percentage"])

# Format the percentages to 2 decimal places
percentages_df["Percentage"] = percentages_df["Percentage"].apply(lambda x: f"{x:.5f}%")

# Save the percentages to a separate file
percentages_df.to_csv("1_10_cross_validation_percentages.tab", sep="\t", index=False)

import pandas as pd
import numpy as np
import itertools
from scipy.stats import pearsonr, t

# Function to calculate Pearson correlation coefficient
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - t.cdf(x, degrees_of_freedom)
    return x, probability

# Step 1: Load the mapped.txt file into a DataFrame
df = pd.read_csv("mapped.txt", header=None, delim_whitespace=True)

# Step 2: Define the specific order of rows (adjusted for 0-based indexing)
row_order = [
    26, 6, 19, 23, 15, 18, 1, 0, 12, 25, 34, 30, 35, 16, 29, 10, 2, 24, 13, 28,
    32, 31, 36, 7, 8, 40, 11, 9, 17, 22, 3, 4, 27, 20, 14, 39, 5, 38, 21, 37, 33
]

# Step 3: Reorder the rows according to the specific row order
df_ordered = df.iloc[row_order].reset_index(drop=True)

# Step 4: Split the 41 rows into 10 groups with the specified sizes
rows_per_group = [4, 4, 4, 4, 4, 4, 4, 4, 4, 5]  # Sizes of the groups
splits = []
start_idx = 0
for count in rows_per_group:
    splits.append(df_ordered.iloc[start_idx:start_idx + count])
    start_idx += count

# Step 5: Generate all combinations of 9 groups out of the 10 groups
group_combinations = list(itertools.combinations(range(10), 9))  # Generate combinations of 9 out of 10 groups

# Step 6: Iterate over each group combination and process
for combo_index, combo in enumerate(group_combinations):
    # Combine the selected groups
    combined_rows = pd.concat([splits[i] for i in combo], ignore_index=True)

    # Save the combined group rows to a file
    group_file_name = f"cross-combo{combo_index + 1}.tab"
    combined_rows.to_csv(group_file_name, sep='\t', index=False, header=False)

    # Step 7: Load the enzyme abundance data (release file)
    release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

    # Step 8: Read the sample names and pollution values
    sample_names = combined_rows[1].values  # Sample names from the combination file
    y_values = combined_rows[2].values  # Pollution values from the combination file

    # Step 9: Create a dictionary of enzyme data from the TARA file
    tara_dict = release_df.set_index('ko').to_dict('index')

    # Step 10: Process the combined rows for correlation calculation
    x_values_for_all_enzymes = []  # List to store x_values for all enzymes for this combination

    # Step 11: Iterate over each enzyme in the TARA file
    for enzyme, enzyme_data in tara_dict.items():
        x_values = []

        # Extract x_values corresponding to the sample names in the current group
        for sample in sample_names:
            if sample in enzyme_data:
                x_values.append(enzyme_data[sample])
            else:
                x_values.append(np.nan)  # If the sample is not found, append NaN

        # Store the x_values along with the enzyme name for future output
        x_values_for_all_enzymes.append([enzyme] + x_values)

    # Step 12: Calculate the Pearson correlation for each enzyme
    correlation_results = []
    for enzyme_data in x_values_for_all_enzymes:
        enzyme_name = enzyme_data[0]
        x_values = enzyme_data[1:]

        # Calculate the correlation between x_values (enzyme abundance) and y_values (pollution values)
        correlation_coefficient = calculate_correlation(x_values, y_values)
        correlation_results.append((enzyme_name, correlation_coefficient))

    # Step 13: Sort the correlation coefficients in descending order
    correlation_results.sort(key=lambda x: x[1], reverse=True)

    # Step 14: Save the sorted correlation results to a file
    with open(f"cross-combo{combo_index + 1}_all_correlations.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient in correlation_results:
            degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
            t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

    # Step 15: Optionally calculate and save the t-value distribution for the correlations
    degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
    significant_rows = []
    total_probability = 0

    # Check the significance of each enzyme
    for enzyme_name, correlation_coefficient in correlation_results:
        t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        # Add the current result if the total probability does not exceed 1
        if total_probability + p_value <= 1:
            significant_rows.append((enzyme_name, correlation_coefficient, t_value, p_value))
            total_probability += p_value
        else:
            break

    # Step 16: Save the significant enzymes to a file
    with open(f"2_10_cross_sig_combo{combo_index + 1}.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient, t_value, p_value in significant_rows:
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# Updated list of cross_sig_combo files for 10 groups
group_files = [
    "2_10_cross_sig_combo1.tab",
    "2_10_cross_sig_combo2.tab",
    "2_10_cross_sig_combo3.tab",
    "2_10_cross_sig_combo4.tab",
    "2_10_cross_sig_combo5.tab",
    "2_10_cross_sig_combo6.tab",
    "2_10_cross_sig_combo7.tab",
    "2_10_cross_sig_combo8.tab",
    "2_10_cross_sig_combo9.tab",
    "2_10_cross_sig_combo10.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 11)]  # Update for 10 groups
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 11)])  # Updated for 10 groups

# Calculate the percentage of common enzymes for each group
percentages = {}
for i in range(1, 11):  # Now looping over 10 groups
    group_column = f"Group{i} Correlation"

    # Count the common enzymes in the current group
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()

    # Get the size of the current group and sig_enzymes
    total_enzymes_in_sig = len(sig_enzymes)
    total_enzymes_in_group = len(df)

    # Calculate the smaller number to divide by
    denominator = min(total_enzymes_in_sig, total_enzymes_in_group)

    # Calculate the percentage common
    percentage_common = (common_enzymes_in_group / denominator) * 100 if denominator else 0

    # Store the percentage in the dictionary for output
    percentages[f"Group{i}"] = percentage_common

# Convert the percentages dictionary into a DataFrame
percentages_df = pd.DataFrame(list(percentages.items()), columns=["Group", "Percentage"])

# Format the percentages to 2 decimal places
percentages_df["Percentage"] = percentages_df["Percentage"].apply(lambda x: f"{x:.5f}%")

# Save the percentages to a separate file
percentages_df.to_csv("2_10_cross_validation_percentages.tab", sep="\t", index=False)

import pandas as pd
import numpy as np
import itertools
from scipy.stats import pearsonr, t

# Function to calculate Pearson correlation coefficient
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - t.cdf(x, degrees_of_freedom)
    return x, probability

# Step 1: Load the mapped.txt file into a DataFrame
df = pd.read_csv("mapped.txt", header=None, delim_whitespace=True)

# Step 2: Define the specific order of rows (adjusted for 0-based indexing)
row_order = [
    37, 15, 14, 1, 0, 2, 8, 16, 19, 30, 4, 34, 26, 11, 20, 7, 3, 33, 29, 28,
    36, 32, 9, 13, 18, 22, 25, 31, 17, 5, 27, 40, 23, 38, 10, 6, 24, 39, 21, 35, 12
]

# Step 3: Reorder the rows according to the specific row order
df_ordered = df.iloc[row_order].reset_index(drop=True)

# Step 4: Split the 41 rows into 10 groups with the specified sizes
rows_per_group = [4, 4, 4, 4, 4, 4, 4, 4, 4, 5]  # Sizes of the groups
splits = []
start_idx = 0
for count in rows_per_group:
    splits.append(df_ordered.iloc[start_idx:start_idx + count])
    start_idx += count

# Step 5: Generate all combinations of 9 groups out of the 10 groups
group_combinations = list(itertools.combinations(range(10), 9))  # Generate combinations of 9 out of 10 groups

# Step 6: Iterate over each group combination and process
for combo_index, combo in enumerate(group_combinations):
    # Combine the selected groups
    combined_rows = pd.concat([splits[i] for i in combo], ignore_index=True)

    # Save the combined group rows to a file
    group_file_name = f"cross-combo{combo_index + 1}.tab"
    combined_rows.to_csv(group_file_name, sep='\t', index=False, header=False)

    # Step 7: Load the enzyme abundance data (release file)
    release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

    # Step 8: Read the sample names and pollution values
    sample_names = combined_rows[1].values  # Sample names from the combination file
    y_values = combined_rows[2].values  # Pollution values from the combination file

    # Step 9: Create a dictionary of enzyme data from the TARA file
    tara_dict = release_df.set_index('ko').to_dict('index')

    # Step 10: Process the combined rows for correlation calculation
    x_values_for_all_enzymes = []  # List to store x_values for all enzymes for this combination

    # Step 11: Iterate over each enzyme in the TARA file
    for enzyme, enzyme_data in tara_dict.items():
        x_values = []

        # Extract x_values corresponding to the sample names in the current group
        for sample in sample_names:
            if sample in enzyme_data:
                x_values.append(enzyme_data[sample])
            else:
                x_values.append(np.nan)  # If the sample is not found, append NaN

        # Store the x_values along with the enzyme name for future output
        x_values_for_all_enzymes.append([enzyme] + x_values)

    # Step 12: Calculate the Pearson correlation for each enzyme
    correlation_results = []
    for enzyme_data in x_values_for_all_enzymes:
        enzyme_name = enzyme_data[0]
        x_values = enzyme_data[1:]

        # Calculate the correlation between x_values (enzyme abundance) and y_values (pollution values)
        correlation_coefficient = calculate_correlation(x_values, y_values)
        correlation_results.append((enzyme_name, correlation_coefficient))

    # Step 13: Sort the correlation coefficients in descending order
    correlation_results.sort(key=lambda x: x[1], reverse=True)

    # Step 14: Save the sorted correlation results to a file
    with open(f"cross-combo{combo_index + 1}_all_correlations.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient in correlation_results:
            degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
            t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

    # Step 15: Optionally calculate and save the t-value distribution for the correlations
    degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
    significant_rows = []
    total_probability = 0

    # Check the significance of each enzyme
    for enzyme_name, correlation_coefficient in correlation_results:
        t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        # Add the current result if the total probability does not exceed 1
        if total_probability + p_value <= 1:
            significant_rows.append((enzyme_name, correlation_coefficient, t_value, p_value))
            total_probability += p_value
        else:
            break

    # Step 16: Save the significant enzymes to a file
    with open(f"3_10_cross_sig_combo{combo_index + 1}.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient, t_value, p_value in significant_rows:
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# Updated list of cross_sig_combo files for 10 groups
group_files = [
    "3_10_cross_sig_combo1.tab",
    "3_10_cross_sig_combo2.tab",
    "3_10_cross_sig_combo3.tab",
    "3_10_cross_sig_combo4.tab",
    "3_10_cross_sig_combo5.tab",
    "3_10_cross_sig_combo6.tab",
    "3_10_cross_sig_combo7.tab",
    "3_10_cross_sig_combo8.tab",
    "3_10_cross_sig_combo9.tab",
    "3_10_cross_sig_combo10.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 11)]  # Update for 10 groups
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 11)])  # Updated for 10 groups

# Calculate the percentage of common enzymes for each group
percentages = {}
for i in range(1, 11):  # Now looping over 10 groups
    group_column = f"Group{i} Correlation"

    # Count the common enzymes in the current group
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()

    # Get the size of the current group and sig_enzymes
    total_enzymes_in_sig = len(sig_enzymes)
    total_enzymes_in_group = len(df)

    # Calculate the smaller number to divide by
    denominator = min(total_enzymes_in_sig, total_enzymes_in_group)

    # Calculate the percentage common
    percentage_common = (common_enzymes_in_group / denominator) * 100 if denominator else 0

    # Store the percentage in the dictionary for output
    percentages[f"Group{i}"] = percentage_common

# Convert the percentages dictionary into a DataFrame
percentages_df = pd.DataFrame(list(percentages.items()), columns=["Group", "Percentage"])

# Format the percentages to 2 decimal places
percentages_df["Percentage"] = percentages_df["Percentage"].apply(lambda x: f"{x:.5f}%")

# Save the percentages to a separate file
percentages_df.to_csv("3_10_cross_validation_percentages.tab", sep="\t", index=False)

import pandas as pd
import numpy as np
import itertools
from scipy.stats import pearsonr, t

# Function to calculate Pearson correlation coefficient
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - t.cdf(x, degrees_of_freedom)
    return x, probability

# Step 1: Load the mapped.txt file into a DataFrame
df = pd.read_csv("mapped.txt", header=None, delim_whitespace=True)

# Step 2: Define the specific order of rows (adjusted for 0-based indexing)
row_order = [
    6, 35, 39, 8, 26, 14, 5, 3, 12, 37, 24, 40, 30,
    13, 4, 29, 19, 28, 9, 7, 0, 27, 11, 15, 18, 22, 1, 17, 20,
    32, 33, 36, 23, 34, 16, 31, 10, 2, 38, 25, 21
 ]

# Step 3: Reorder the rows according to the specific row order
df_ordered = df.iloc[row_order].reset_index(drop=True)

# Step 4: Split the 41 rows into 10 groups with the specified sizes
rows_per_group = [4, 4, 4, 4, 4, 4, 4, 4, 4, 5]  # Sizes of the groups
splits = []
start_idx = 0
for count in rows_per_group:
    splits.append(df_ordered.iloc[start_idx:start_idx + count])
    start_idx += count

# Step 5: Generate all combinations of 9 groups out of the 10 groups
group_combinations = list(itertools.combinations(range(10), 9))  # Generate combinations of 9 out of 10 groups

# Step 6: Iterate over each group combination and process
for combo_index, combo in enumerate(group_combinations):
    # Combine the selected groups
    combined_rows = pd.concat([splits[i] for i in combo], ignore_index=True)

    # Save the combined group rows to a file
    group_file_name = f"cross-combo{combo_index + 1}.tab"
    combined_rows.to_csv(group_file_name, sep='\t', index=False, header=False)

    # Step 7: Load the enzyme abundance data (release file)
    release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

    # Step 8: Read the sample names and pollution values
    sample_names = combined_rows[1].values  # Sample names from the combination file
    y_values = combined_rows[2].values  # Pollution values from the combination file

    # Step 9: Create a dictionary of enzyme data from the TARA file
    tara_dict = release_df.set_index('ko').to_dict('index')

    # Step 10: Process the combined rows for correlation calculation
    x_values_for_all_enzymes = []  # List to store x_values for all enzymes for this combination

    # Step 11: Iterate over each enzyme in the TARA file
    for enzyme, enzyme_data in tara_dict.items():
        x_values = []

        # Extract x_values corresponding to the sample names in the current group
        for sample in sample_names:
            if sample in enzyme_data:
                x_values.append(enzyme_data[sample])
            else:
                x_values.append(np.nan)  # If the sample is not found, append NaN

        # Store the x_values along with the enzyme name for future output
        x_values_for_all_enzymes.append([enzyme] + x_values)

    # Step 12: Calculate the Pearson correlation for each enzyme
    correlation_results = []
    for enzyme_data in x_values_for_all_enzymes:
        enzyme_name = enzyme_data[0]
        x_values = enzyme_data[1:]

        # Calculate the correlation between x_values (enzyme abundance) and y_values (pollution values)
        correlation_coefficient = calculate_correlation(x_values, y_values)
        correlation_results.append((enzyme_name, correlation_coefficient))

    # Step 13: Sort the correlation coefficients in descending order
    correlation_results.sort(key=lambda x: x[1], reverse=True)

    # Step 14: Save the sorted correlation results to a file
    with open(f"cross-combo{combo_index + 1}_all_correlations.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient in correlation_results:
            degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
            t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

    # Step 15: Optionally calculate and save the t-value distribution for the correlations
    degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
    significant_rows = []
    total_probability = 0

    # Check the significance of each enzyme
    for enzyme_name, correlation_coefficient in correlation_results:
        t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        # Add the current result if the total probability does not exceed 1
        if total_probability + p_value <= 1:
            significant_rows.append((enzyme_name, correlation_coefficient, t_value, p_value))
            total_probability += p_value
        else:
            break

    # Step 16: Save the significant enzymes to a file
    with open(f"4_10_cross_sig_combo{combo_index + 1}.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient, t_value, p_value in significant_rows:
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# Updated list of cross_sig_combo files for 10 groups
group_files = [
    "4_10_cross_sig_combo1.tab",
    "4_10_cross_sig_combo2.tab",
    "4_10_cross_sig_combo3.tab",
    "4_10_cross_sig_combo4.tab",
    "4_10_cross_sig_combo5.tab",
    "4_10_cross_sig_combo6.tab",
    "4_10_cross_sig_combo7.tab",
    "4_10_cross_sig_combo8.tab",
    "4_10_cross_sig_combo9.tab",
    "4_10_cross_sig_combo10.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 11)]  # Update for 10 groups
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 11)])  # Updated for 10 groups

# Calculate the percentage of common enzymes for each group
percentages = {}
for i in range(1, 11):  # Now looping over 10 groups
    group_column = f"Group{i} Correlation"

    # Count the common enzymes in the current group
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()

    # Get the size of the current group and sig_enzymes
    total_enzymes_in_sig = len(sig_enzymes)
    total_enzymes_in_group = len(df)

    # Calculate the smaller number to divide by
    denominator = min(total_enzymes_in_sig, total_enzymes_in_group)

    # Calculate the percentage common
    percentage_common = (common_enzymes_in_group / denominator) * 100 if denominator else 0

    # Store the percentage in the dictionary for output
    percentages[f"Group{i}"] = percentage_common

# Convert the percentages dictionary into a DataFrame
percentages_df = pd.DataFrame(list(percentages.items()), columns=["Group", "Percentage"])

# Format the percentages to 2 decimal places
percentages_df["Percentage"] = percentages_df["Percentage"].apply(lambda x: f"{x:.5f}%")

# Save the percentages to a separate file
percentages_df.to_csv("4_10_cross_validation_percentages.tab", sep="\t", index=False)

import pandas as pd
import numpy as np
import itertools
from scipy.stats import pearsonr, t

# Function to calculate Pearson correlation coefficient
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - t.cdf(x, degrees_of_freedom)
    return x, probability

# Step 1: Load the mapped.txt file into a DataFrame
df = pd.read_csv("mapped.txt", header=None, delim_whitespace=True)

# Step 2: Define the specific order of rows (adjusted for 0-based indexing)
row_order = [
    17, 32, 31, 7, 6, 26, 8, 19, 28, 10, 1, 33, 35, 2,
    14, 5, 34, 39, 22, 4, 13, 21, 38, 18, 16, 0, 9, 12, 15,
    27, 23, 36, 3, 37, 20, 11, 40, 25, 24, 30, 29
 ]

# Step 3: Reorder the rows according to the specific row order
df_ordered = df.iloc[row_order].reset_index(drop=True)

# Step 4: Split the 41 rows into 10 groups with the specified sizes
rows_per_group = [4, 4, 4, 4, 4, 4, 4, 4, 4, 5]  # Sizes of the groups
splits = []
start_idx = 0
for count in rows_per_group:
    splits.append(df_ordered.iloc[start_idx:start_idx + count])
    start_idx += count

# Step 5: Generate all combinations of 9 groups out of the 10 groups
group_combinations = list(itertools.combinations(range(10), 9))  # Generate combinations of 9 out of 10 groups

# Step 6: Iterate over each group combination and process
for combo_index, combo in enumerate(group_combinations):
    # Combine the selected groups
    combined_rows = pd.concat([splits[i] for i in combo], ignore_index=True)

    # Save the combined group rows to a file
    group_file_name = f"cross-combo{combo_index + 1}.tab"
    combined_rows.to_csv(group_file_name, sep='\t', index=False, header=False)

    # Step 7: Load the enzyme abundance data (release file)
    release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

    # Step 8: Read the sample names and pollution values
    sample_names = combined_rows[1].values  # Sample names from the combination file
    y_values = combined_rows[2].values  # Pollution values from the combination file

    # Step 9: Create a dictionary of enzyme data from the TARA file
    tara_dict = release_df.set_index('ko').to_dict('index')

    # Step 10: Process the combined rows for correlation calculation
    x_values_for_all_enzymes = []  # List to store x_values for all enzymes for this combination

    # Step 11: Iterate over each enzyme in the TARA file
    for enzyme, enzyme_data in tara_dict.items():
        x_values = []

        # Extract x_values corresponding to the sample names in the current group
        for sample in sample_names:
            if sample in enzyme_data:
                x_values.append(enzyme_data[sample])
            else:
                x_values.append(np.nan)  # If the sample is not found, append NaN

        # Store the x_values along with the enzyme name for future output
        x_values_for_all_enzymes.append([enzyme] + x_values)

    # Step 12: Calculate the Pearson correlation for each enzyme
    correlation_results = []
    for enzyme_data in x_values_for_all_enzymes:
        enzyme_name = enzyme_data[0]
        x_values = enzyme_data[1:]

        # Calculate the correlation between x_values (enzyme abundance) and y_values (pollution values)
        correlation_coefficient = calculate_correlation(x_values, y_values)
        correlation_results.append((enzyme_name, correlation_coefficient))

    # Step 13: Sort the correlation coefficients in descending order
    correlation_results.sort(key=lambda x: x[1], reverse=True)

    # Step 14: Save the sorted correlation results to a file
    with open(f"cross-combo{combo_index + 1}_all_correlations.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient in correlation_results:
            degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
            t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

    # Step 15: Optionally calculate and save the t-value distribution for the correlations
    degrees_of_freedom = len(combined_rows) - 2  # Adjust degrees of freedom based on current group size
    significant_rows = []
    total_probability = 0

    # Check the significance of each enzyme
    for enzyme_name, correlation_coefficient in correlation_results:
        t_value, p_value = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

        # Add the current result if the total probability does not exceed 1
        if total_probability + p_value <= 1:
            significant_rows.append((enzyme_name, correlation_coefficient, t_value, p_value))
            total_probability += p_value
        else:
            break

    # Step 16: Save the significant enzymes to a file
    with open(f"5_10_cross_sig_combo{combo_index + 1}.tab", "w") as file:
        file.write("Enzyme\tCorrelation\tT-dist\tP-value\n")
        for enzyme_name, correlation_coefficient, t_value, p_value in significant_rows:
            file.write(f"{enzyme_name}\t{correlation_coefficient}\t{t_value}\t{p_value}\n")

import pandas as pd

# Function to read the enzyme data from the sig_pearson_enzymes.txt file
def read_sig_pearson_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split(", ")
                enzyme_name = parts[0].split(" = ")[1]
                correlation = float(parts[1].split(" = ")[1])
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

# Function to read the enzyme data from cross_sig_combo files
def read_cross_sig_file(file_name):
    enzyme_data = {}
    with open(file_name, "r") as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():  # Skip empty lines
                parts = line.strip().split("\t")
                enzyme_name = parts[0]  # Enzyme name is in the first column
                correlation = float(parts[1])  # Correlation is in the second column
                enzyme_data[enzyme_name] = correlation
    return enzyme_data

sig_enzymes = read_sig_pearson_file("sig_pearson_enzymes.txt")

# Updated list of cross_sig_combo files for 10 groups
group_files = [
    "5_10_cross_sig_combo1.tab",
    "5_10_cross_sig_combo2.tab",
    "5_10_cross_sig_combo3.tab",
    "5_10_cross_sig_combo4.tab",
    "5_10_cross_sig_combo5.tab",
    "5_10_cross_sig_combo6.tab",
    "5_10_cross_sig_combo7.tab",
    "5_10_cross_sig_combo8.tab",
    "5_10_cross_sig_combo9.tab",
    "5_10_cross_sig_combo10.tab"
]

# Create a dictionary to store which groups share which enzymes
shared_enzyme_data = {}

# Process each cross_sig_combo file
for i, group_file in enumerate(group_files, start=1):
    group_enzymes = read_cross_sig_file(group_file)
    for enzyme_name in group_enzymes:
        if enzyme_name in sig_enzymes:  # Only consider enzymes that are shared between sig_pearson and the current group
            if enzyme_name not in shared_enzyme_data:
                shared_enzyme_data[enzyme_name] = {"Enzyme Name": enzyme_name}

            # Add the correlation for the specific group
            shared_enzyme_data[enzyme_name][f"Group{i} Correlation"] = group_enzymes[enzyme_name]

# Convert shared enzyme data into a DataFrame
final_data = []
for enzyme_name, group_data in shared_enzyme_data.items():
    row = [enzyme_name] + [group_data.get(f"Group{i} Correlation", "na") for i in range(1, 11)]  # Update for 10 groups
    final_data.append(row)

# Create DataFrame
df = pd.DataFrame(final_data, columns=["Enzyme Name"] + [f"Group{i} Correlation" for i in range(1, 11)])  # Updated for 10 groups

# Calculate the percentage of common enzymes for each group
percentages = {}
for i in range(1, 11):  # Now looping over 10 groups
    group_column = f"Group{i} Correlation"

    # Count the common enzymes in the current group
    common_enzymes_in_group = df[group_column].apply(lambda x: x != "na").sum()

    # Get the size of the current group and sig_enzymes
    total_enzymes_in_sig = len(sig_enzymes)
    total_enzymes_in_group = len(df)

    # Calculate the smaller number to divide by
    denominator = min(total_enzymes_in_sig, total_enzymes_in_group)

    # Calculate the percentage common
    percentage_common = (common_enzymes_in_group / denominator) * 100 if denominator else 0

    # Store the percentage in the dictionary for output
    percentages[f"Group{i}"] = percentage_common

# Convert the percentages dictionary into a DataFrame
percentages_df = pd.DataFrame(list(percentages.items()), columns=["Group", "Percentage"])

# Format the percentages to 2 decimal places
percentages_df["Percentage"] = percentages_df["Percentage"].apply(lambda x: f"{x:.5f}%")

# Save the percentages to a separate file
percentages_df.to_csv("5_10_cross_validation_percentages.tab", sep="\t", index=False)

"""# Confidence Interval"""

import numpy as np
import scipy.stats as stats
import pandas as pd
from scipy.stats import pearsonr

# Function to calculate correlation
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = pearsonr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - stats.t.cdf(x, degrees_of_freedom)
    return x, probability

# Degrees of freedom for the t-distribution
degrees_of_freedom = 39

# Load the species abundance data (release file)
release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

# Load the mapped data with corresponding pollution values
mapped_data = pd.read_csv('mapped.txt', sep='\t', header=None)

# Read all the second values (sample IDs) and y values (pollution values)
second_values = mapped_data[1].values
y_values = mapped_data[2].values

# Dictionary to store correlation coefficients and x values for each species
correlation_coefficients = {}
x_values_for_all_species = []  # List to store x_values for all species

# Iterate over each row of the species abundance data
for row_index in range(len(release_df)):
    # Extract the x_values corresponding to each sample (second_value)
    x_values = [release_df.iloc[row_index][str(second_value)] for second_value in second_values]

    # Resize y_values to match the length of x_values
    desired_length = len(x_values)
    y_values_resized = np.resize(y_values, desired_length)

    # Calculate the correlation coefficient for this species
    correlation_coefficient = calculate_correlation(x_values, y_values_resized)
    correlation_coefficients[row_index] = correlation_coefficient

    # Store the x_values along with the species name for future output
    species_name = release_df.iloc[row_index, 0]  # Assuming the first column contains species names
    x_values_for_all_species.append([species_name] + x_values)

# Sort the correlation coefficients in descending order
sorted_coefficients = sorted(correlation_coefficients.items(), key=lambda x: x[1], reverse=True)

# Create a list of the first column values (species names) from the release file
first_column_values = release_df.iloc[:, 0].values

# Now calculate the t-value and p-value for the sorted coefficients and prepare for output
significant_rows = []
total_probability = 0

for row_index, correlation_coefficient in sorted_coefficients:
    first_value = first_column_values[row_index]  # Get the corresponding species name
    x, probability = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

    # Check if adding the current probability exceeds 1
    if total_probability + probability <= 1:
        significant_rows.append((row_index, first_value, correlation_coefficient, x, probability))
        total_probability += probability
    else:
        break

with open("sig_pearson_enzymes.txt", "w") as file:
    for row_index, first_value, correlation_coefficient, x, probability in significant_rows:
        file.write(f"species = {first_value}, Correlation = {correlation_coefficient}, t-dist = {x}, p-value = {probability}\n")

import numpy as np
import scipy.stats as stats

# Function to perform Fisher's transformation
def fisher_z_transformation(correlation_coefficient):
    return 0.5 * np.log((1 + correlation_coefficient) / (1 - correlation_coefficient))

# Function to calculate confidence interval for correlation
def calculate_confidence_interval(correlation_coefficient, degrees_of_freedom, confidence_level=0.95):
    # Step 1: Apply Fisher's transformation
    z = fisher_z_transformation(correlation_coefficient)

    # Step 2: Calculate the standard error (SE)
    n = degrees_of_freedom + 2  # Sample size (degrees of freedom + 2)
    se = 1 / np.sqrt(n - 3)

    # Step 3: Calculate the Z-value for the confidence level
    alpha = 1 - confidence_level
    z_alpha = stats.norm.ppf(1 - alpha / 2)  # Z value for confidence level (e.g., 1.96 for 95% CI)

    # Step 4: Compute the confidence interval in the Fisher transformed space
    z_lower = z - z_alpha * se
    z_upper = z + z_alpha * se

    # Step 5: Back-transform the Z interval to get the correlation coefficient interval
    r_lower = (np.exp(2 * z_lower) - 1) / (np.exp(2 * z_lower) + 1)
    r_upper = (np.exp(2 * z_upper) - 1) / (np.exp(2 * z_upper) + 1)

    return r_lower, r_upper

significant_rows = []
with open("sig_pearson_enzymes.txt", "r") as file:
    for line in file:
        # Parse the enzyme name, correlation coefficient, and t-dist values
        parts = line.strip().split(", ")
        enzyme_name = parts[0].split(" = ")[1]
        correlation = float(parts[1].split(" = ")[1])
        t_value = float(parts[2].split(" = ")[1].split()[0])
        p_value = float(parts[3].split(" = ")[1])

        significant_rows.append((enzyme_name, correlation, t_value, p_value))

# Calculate the confidence intervals and store the results
confidence_intervals = []
for enzyme_name, correlation, t_value, p_value in significant_rows:
    df = 39

    # Calculate the confidence interval for this enzyme's correlation coefficient
    r_lower, r_upper = calculate_confidence_interval(correlation, df)

    confidence_intervals.append((enzyme_name, correlation, r_lower, r_upper, t_value, p_value))

# Write to a .tab file with header
with open("confidence_intervals_pearson.tab", "w") as file:
    # Write header row
    file.write("Enzyme\tCorrelation\t95% CI Lower\t95% CI Upper\tt-dist (x value)\tp-value\n")

    # Write data rows without "Enzyme = " etc.
    for enzyme_name, correlation, r_lower, r_upper, t_value, p_value in confidence_intervals:
        # Using tabs as separators instead of the " = " syntax
        file.write(f"{enzyme_name}\t{correlation}\t{r_lower}\t{r_upper}\t{t_value}\t{p_value}\n")

"""### Confidence Interval (Spearman)"""

import numpy as np
import scipy.stats as stats
import pandas as pd
from scipy.stats import spearmanr

# Function to calculate correlation
def calculate_correlation(x_values, y_values):
    correlation_coefficient, _ = spearmanr(x_values, y_values)
    return correlation_coefficient

# Function to calculate t-value distribution
def calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom):
    x = correlation_coefficient * np.sqrt(degrees_of_freedom) / np.sqrt(1 - correlation_coefficient**2)
    probability = 1 - stats.t.cdf(x, degrees_of_freedom)
    return x, probability

# Degrees of freedom for the t-distribution
degrees_of_freedom = 39

# Load the enzyme abundance data (release file)
release_df = pd.read_csv("TARA243.KO.profile.release", delim_whitespace=True, skiprows=[1])

# Load the mapped data with corresponding pollution values
mapped_data = pd.read_csv('mapped.txt', sep='\t', header=None)

# Read all the second values (sample IDs) and y values (pollution values)
second_values = mapped_data[1].values
y_values = mapped_data[2].values

# Dictionary to store correlation coefficients and x values for each enzyme
correlation_coefficients = {}
x_values_for_all_enzymes = []  # List to store x_values for all enzymes

# Iterate over each row of the enzyme abundance data
for row_index in range(len(release_df)):
    # Extract the x_values corresponding to each sample (second_value)
    x_values = [release_df.iloc[row_index][str(second_value)] for second_value in second_values]

    # Resize y_values to match the length of x_values
    desired_length = len(x_values)
    y_values_resized = np.resize(y_values, desired_length)

    # Calculate the correlation coefficient for this enzyme
    correlation_coefficient = calculate_correlation(x_values, y_values_resized)
    correlation_coefficients[row_index] = correlation_coefficient

    # Store the x_values along with the enzyme name for future output
    enzyme_name = release_df.iloc[row_index, 0]  # Assuming the first column contains enzyme names
    x_values_for_all_enzymes.append([enzyme_name] + x_values)

# Sort the correlation coefficients in descending order
sorted_coefficients = sorted(correlation_coefficients.items(), key=lambda x: x[1], reverse=True)

# Create a list of the first column values (enzyme names) from the release file
first_column_values = release_df.iloc[:, 0].values

# Now calculate the t-value and p-value for the sorted coefficients and prepare for output
significant_rows = []
total_probability = 0

for row_index, correlation_coefficient in sorted_coefficients:
    first_value = first_column_values[row_index]  # Get the corresponding enzyme name
    x, probability = calculate_t_value_distribution(correlation_coefficient, degrees_of_freedom)

    # Check if adding the current probability exceeds 1
    if total_probability + probability <= 1:
        significant_rows.append((row_index, first_value, correlation_coefficient, x, probability))
        total_probability += probability
    else:
        break

with open("sig_spearman_enzymes.txt", "w") as file:
    for row_index, first_value, correlation_coefficient, x, probability in significant_rows:
        file.write(f"Enzyme = {first_value}, Correlation = {correlation_coefficient}, t-dist = {x}, p-value = {probability}\n")

import numpy as np
import scipy.stats as stats

# Function to perform Fisher's transformation
def fisher_z_transformation(correlation_coefficient):
    return 0.5 * np.log((1 + correlation_coefficient) / (1 - correlation_coefficient))

# Function to calculate confidence interval for correlation
def calculate_confidence_interval(correlation_coefficient, degrees_of_freedom, confidence_level=0.95):
    # Step 1: Apply Fisher's transformation
    z = fisher_z_transformation(correlation_coefficient)

    # Step 2: Calculate the standard error (SE)
    n = degrees_of_freedom + 2  # Sample size (degrees of freedom + 2)
    se = 1 / np.sqrt(n - 3)

    # Step 3: Calculate the Z-value for the confidence level
    alpha = 1 - confidence_level
    z_alpha = stats.norm.ppf(1 - alpha / 2)  # Z value for confidence level (e.g., 1.96 for 95% CI)

    # Step 4: Compute the confidence interval in the Fisher transformed space
    z_lower = z - z_alpha * se
    z_upper = z + z_alpha * se

    # Step 5: Back-transform the Z interval to get the correlation coefficient interval
    r_lower = (np.exp(2 * z_lower) - 1) / (np.exp(2 * z_lower) + 1)
    r_upper = (np.exp(2 * z_upper) - 1) / (np.exp(2 * z_upper) + 1)

    return r_lower, r_upper

significant_rows = []
with open("sig_spearman_enzymes.txt", "r") as file:
    for line in file:
        # Parse the enzyme name, correlation coefficient, and t-dist values
        parts = line.strip().split(", ")
        enzyme_name = parts[0].split(" = ")[1]
        correlation = float(parts[1].split(" = ")[1])
        t_value = float(parts[2].split(" = ")[1].split()[0])
        p_value = float(parts[3].split(" = ")[1])

        significant_rows.append((enzyme_name, correlation, t_value, p_value))

# Calculate the confidence intervals and store the results
confidence_intervals = []
for enzyme_name, correlation, t_value, p_value in significant_rows:
    df = 39

    # Calculate the confidence interval for this enzyme's correlation coefficient
    r_lower, r_upper = calculate_confidence_interval(correlation, df)

    confidence_intervals.append((enzyme_name, correlation, r_lower, r_upper, t_value, p_value))

# Change the output file extension to .tab
with open("confidence_intervals_spearman.tab", "w") as file:
    # Write header row
    file.write("Enzyme\tCorrelation\t95% CI Lower\t95% CI Upper\tt-dist (x value)\tp-value\n")

    # Write data rows without the "Enzyme = " etc.
    for enzyme_name, correlation, r_lower, r_upper, t_value, p_value in confidence_intervals:
        # Using tabs as separators instead of the " = " syntax
        file.write(f"{enzyme_name}\t{correlation}\t{r_lower}\t{r_upper}\t{t_value}\t{p_value}\n")